## Results

Format:

We wanted to find out to find X
So we did Y
We found Z

### Approach (or Constructing classifiers for expression data)
To compare linear and nonlinear models, we first needed a consistent way to train them and determine their performance.
Because model performance is dependent on both model initialization and training data, we decided to use several instances of both.
On the data side, the we split the data via fivefold cross-validation to evaluate each model on multiple training and testing sets.
In order to avoid leakage between folds, the studies were placed in their entirety into a fold instead of being split across folds.
We also run the models on increasingly large subsets of the training data to determine how model performance is affected by the amount of training data.
To ensure that artifacts specific to a single data split or model initialization don't drive the signal, we run each of our experiments with three different random seeds.
As a results of these different dimensions of variation, each evaluation we perform reflects 150 trained instances of each model.

- Maybe make a diagram?

### Linear and nonlinear models perform equivalently on metadata prediction tasks
To determine whether linear or nonlinear models 

- Binary tissue classification
- 21 class tissue classification

### This isn't because there is only linear signal in the data

- Pairwise tissue classification with label signal removed

### This isn't because tissue prediction is special

- Sex prediction

### This isn't because there is an obvious issue with the dataset

- Confusion matrix
- QC plot

### This is still true when there is data leakage as in common data splitting formats

- Maybe one sample-level plot, but mostly should be supplementary figures

### Pretraining doesn't save things (is this necessary?)

- Pretraining plots
