## Results 

Format:

We wanted to find out to find X
So we did Y
We found Z

### Approach (or Constructing classifiers for expression data)
To compare linear and nonlinear models, we first needed a consistent way to train them and determine their performance.
Because model performance is dependent on both model initialization and training data, we decided to use several instances of both.
On the data side, the we split the data via fivefold cross-validation to evaluate each model on multiple training and testing sets.
In order to avoid leakage between folds, the studies were placed in their entirety into a fold instead of being split across folds.
We also run the models on increasingly large subsets of the training data to determine how model performance is affected by the amount of training data.
To ensure that artifacts specific to a single data split or model initialization don't drive the signal, we run each of our experiments with three different random seeds.
As a results of these different dimensions of variation, each evaluation we perform reflects 150 trained instances of each model.

- Maybe make a diagram?

### Linear and nonlinear models perform equivalently on metadata prediction tasks
To determine whether linear or nonlinear models performed similarly, we set them to a number of tissue prediction tasks.
First we compared their abilities to differentiate between pairs of tissues (Fig. @fig:binary-classification).
We found that their performance was roughly equivalent.
More specifically, given tissue pairs seem to have maximum accuracy thresholds that models achieve and then stagnate.

In case the accuracy thresholds were due to the relative easiness of the binary classification task, we selected a harder problem.
Namely, we evaluated the models on their ability to predict which of the 21 most common tissues in the dataset a sample belonged to.
For this task, the scikit-learn implementation of logistic regression outperformed the other models, with our Pytorch logistic regression implementation and five layer neural network having similar performance and the three layer neural network performing the worst (Fig. @fig:multiclass).

### This isn't because there is only linear signal in the data
One can imagine a world where all the signal relevant to tissue prediction is linear.
If that were the case, nonlinear models like neural networks would fail to give any advantage in a prediction problem.
To determine whether there is nonlinear signal in our tissue prediction tasks that is learnable by our neural nets, we used Limma (CITE) to remove the linear signal associated with each tissue.
When we ran our models on the signal-removed data, we found X

### This isn't because tissue prediction is special
To rule out the possibility that our findings were specific to tissue prediction tasks, we examined models' ability to predict metadata-derived sex (Fig. @fig:sex-prediction).
Once again we the task showed similar performance between logistic regression and neural nets.

### This isn't because there is an obvious issue with the dataset

- Confusion matrix
- QC plot

### This is still true when there is data leakage as in common data splitting formats

- Maybe one sample-level plot, but mostly should be supplementary figures

### Pretraining doesn't save things (is this necessary?)

- Pretraining plots
