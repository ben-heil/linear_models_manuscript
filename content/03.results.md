## Results 

### Approach (or Constructing classifiers for expression data)
TODO workflow diagram here

Our dataset is composed of gene expression from recount3 [CITE], with tissue labels from the recount3 metadata and sex labels from Flynn et al. [CITE].
Before use in model training, we removed scRNA samples, TPM normalized, and zero-one standardized the data (see Methods for more details).

We split our dataset via fivefold cross-validation to evaluate each model on multiple training and testing sets.
In order to avoid leakage between folds, the studies were placed in their entirety into a fold instead of being split across folds.
We also ran the models on increasingly large subsets of the training data to determine how model performance is affected by the amount of training data.

To ensure that artifacts specific to a single data split or model initialization don't drive the signal, we run each of our experiments with three different random seeds.
As a results of these different dimensions of variation, each evaluation we perform reflects 150 trained instances of each model.

### Linear and nonlinear models perform equivalently on metadata prediction tasks
To determine whether linear or nonlinear models performed similarly, we set them to a number of tissue prediction tasks.
First we compared their abilities to differentiate between pairs of tissues (Fig. @fig:binary-classification).
We found that their performance was roughly equivalent.
More specifically, given tissue pairs seem to have maximum accuracy thresholds that models achieve and then stagnate.

In case the accuracy threshold effects were due to the relative easiness of the binary classification task, we selected a harder problem.
Namely, we evaluated the models on their ability to predict which of the 21 most common tissues in the dataset a sample belonged to.
For this task, the scikit-learn implementation of logistic regression outperformed the other models, with our PyTorch logistic regression implementation and five layer neural network having similar performance and the three layer neural network performing the worst (Fig. @fig:multiclass).
We suspect that this is due to scikit-learn linear regression's optimization method outperforming the stochastic gradient descent used by our other models.

### This isn't because there is only linear signal in the data
One can imagine a world where all the signal relevant to tissue prediction is linear.
If that were the case, nonlinear models like neural networks would fail to give any advantage in a prediction problem.
To determine whether there is nonlinear signal in our tissue prediction tasks that is learnable by our neural nets, we used Limma [@doi:10.1093/nar/gkv007] to remove the linear signal associated with each tissue.
When we ran our models on the signal-removed data, we found that while the neural networks manage to perform better than the random classification baseline, the logistic regression models do worse than the baseline (Fig. @fig:binary-signal-removed).
TODO describe curves
We hypothesize that this underperformance is caused by the linear models fitting to noise that is spuriously correlated with the tissue labels in the training set.

### This isn't because tissue prediction is special
To rule out the possibility that our findings were specific to tissue prediction tasks, we examined models' ability to predict metadata-derived sex (Fig. @fig:sex-prediction).
We used the same experimental setup as in our other binary prediction tasks to train the models, but rather than using two tissues as labels, we used sex.
Once again the task showed similar performance between logistic regression and neural nets.
TODO sentence describing curve

### This is still true when there is data leakage as in common data splitting formats
There is a common method of data splitting we refer to as samplewise splitting (see methods section X) that leaks informatiion between the train and test sets when used in transcriptomic tasks.
To avoid this data leakage, we split the dataset at the study level.
We considered it possible that our results were an artifact of our method of dataset splitting, and set out to test it.
To do so, we ran our multi-tissue analysis using sample splitting and found X.

- Maybe also make supplementary figures of binary classification with sample splitting?

### Pretraining doesn't save things (should include a section on transfer learning?)

- Pretraining plots

### There isn't a strong linear artifact causing this
To determine whether our results were driven by an artifact in the data, we performed exploratory data analysis.
First, we looked for whether anything stood out when comparing per-sample performance between models.
Upon doing so, we found that TODO describe after running on all-tissue
When looking at the results, we noticed that some samples were consistently misclassified across models. 
We suspected it might be due to label imbalance, but a confusion matrix showed that not to be the case.
We examined the metadata for attributes that might be correlated with sample prediction hardness, and found that these samples tend to have a lower read quality than other samples.

