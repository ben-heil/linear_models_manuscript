## Results 

### Approach 
We compared the performance of linear and nonlinear models across a number of datasets and on multiple tasks (fig. @fig:workflow top).
Our datasets consisted of gene expression and tissue labels from GTEx [@doi:10.1038/ng.2653], expression from Recount3 [@doi:10.1186/s13059-021-02533-6] with tissue labels from the Recount3 metadata and sex labels from Flynn et al. [@doi:10.1186/s12859-021-04070-2], and simulated data.
Before use in model training, we removed scRNA samples, TPM normalized, and zero-one standardized the data.
To avoid leakage between cross-validation folds, the studies were placed in their entirety into a fold instead of being split across folds (fig. @fig:workflow bottom).
We then ran the models on increasingly large training sets to determine how model performance is affected by the amount of training data.

![
Schematic of the model analysis workflow. We evaluate three models on multiple classification problems in three datasets (top). To do so, we use studywise splitting as a default, but also evaluate the effects of samplewise splitting and using a pretraining dataset.
](./images/workflow.svg "Workflow diagram"){#fig:workflow}


### Linear and nonlinear models have similar performance in many tasks
We selected expression data from GTEx [@doi:10.1038/ng.2653] to determine whether linear and nonlinear models performed similarly, as it is a fairly well understood dataset with consistent experimental design across samples.
We first trained our models to do binary classification on pairs of the five most common tissues in the dataset.
Likely due to the clean nature of the data, all models were able to perform perfectly on the binary classification tasks (fig. @fig:gtex bottom).

Because the binary classification task was too easy to determine any difference between models, we decided to evaluate the models on a harder task.
Namely, we evaluated the models on their ability to perform multiclass classification on all 31 tissues present in the dataset.
The multitask setting showed logistic regression slightly outperforming the five layer neural network, which in turn slightly outperformed the three layer net (fig. @fig:gtex top).

We then validated our findings in a separate dataset: Sequence Read Archive [@doi:10.1093/nar/gkq1019] samples from the Recount3 [@doi:10.1186/s13059-021-02533-6] dataset.
Again we compared the models' abilities to differentiate between pairs of tissues (fig. @fig:recount-binary), and found that their performance was roughly equivalent.
More specifically, the tissue pairs seemed to have maximum accuracy thresholds that models would achieve and then plateau.
We also evaluated the models' performance on a multiclass classification  problem differentiating between the 21 most common tissues in the dataset.
As in the GTEx setting, the logistic regression model outperformed the five layer network, which outperformed the three layer network (fig. @fig:recount-multiclass).

To examine whether these results held in a problem domain other than tissue type prediction, we used our models to predict metadata-derived sex labels (fig. @fig:sex-prediction), a task previously studied by Flynn et al. [@doi:10.1186/s12859-021-04070-2].
We used the same experimental setup as in our other binary prediction tasks to train the models, but rather than using tissue labels we used metadata-derived sex labels.
In this setting we found that while the models all performed similarly, the nonlinear models tended to have a slight edge over the linear one.

![
Metadata sex prediction
](./images/sex_prediction.svg ){#fig:sex-prediction}

![
Performance of model on GTEx classification problems. The top figures show the difference in training models in the multiclass setting with and without signal removal, while the bottom figures show binary classification with and without signal removal.
](./images/gtex_combined.svg "Gtex classification tasks"){#fig:gtex}

![
Comparison of models' binary classification performance before and after removing linear signal
](./images/recount_binary_combined.svg "Recount binary classification before and after signal removal"){#fig:recount-binary}

![
Graph of the Recount3 multiclass classification results. The dashed line represents the baseline accuracy of random predictions.
](./images/recount_multiclass_combined.svg "Recount multiclass classification"){#fig:recount-multiclass}

### There is predictive nonlinear signal in biological problems
One can imagine a world where all the signal relevant to tissue prediction is linear.
If that were the case, nonlinear models like neural networks would fail to give any advantage in a prediction problem.
To determine whether there is nonlinear signal in our tissue prediction tasks learnable by our neural nets, we used Limma [@doi:10.1093/nar/gkv007] to remove the linear signal associated with each tissue.

We began by simulating three datasets to better understand the patterns we would expect to see for different types of signal.
Our initial dataset simulated both linear and nolinear signal by generating two types of features: half of the features with a linear dividing line between the simulated classes and half with a nonlinear dividing line.
After training to classify the simulated dataset, all models were able to effectively predict the simulated classes.
After removing the linear signal from the dataset, nonlinear models were still able to easily predict the correct classes, but logistic regression was no better than random (fig @fig:simulation middle).

To show what the models' performance would look like in data with only linear signal, we generated another simulated dataset consiting solely of features with a linear dividing line between the classes.
As before, all models were able to predict the different classes well.
However, once the linear signal was removed all models had accuracy no better than random guessing (fig @fig:simulation left).
This also indicated that the signal removal method was not injecting nonlinear signal into data where nonlinear signal did not exist.

We also trained the models on a dataset where all features were gaussian noise as a negative control.
As expected, the models all performed at baseline accuracy both before and after the signal removal process (fig. @fig:simulation right).
This finding supported our decision to remove signal from the training and validation sets separately, as removing the signal in the full dataset may introduce predictive signal into this setting (supp. fig. @fig:split-signal-correction).

![
Performance of models in binary classification of simulated data before and after signal removal
](./images/simulated_data_combined.svg ){#fig:simulation}

When we ran our models on the signal-removed data from GTEx and Recount3, we found that the neural nets performed better than the baseline while logistic regression did not (fig. @fig:recount-binary).
The neural networks performed less well in the low-data regime than when run on data without signal removed, indicating an increase in the difficulty of the problem, and the logistic regression implementation performed no better than random (fig. @fig:gtex bottom).
Similarly, the multiclass problem had the logistic regression model performing poorly, while the nonlinear models had performance that increased with an increase in data while remaining worse than before the linear signal was removed (fig. @fig:gtex top).

TODO figure signal removed gtex/recount/sex prediction

To verify that our results weren't an artifact of how we assigned samples to cross-validation folds, we compared the method we used to assign folds with one called samplewise splitting.
Samplewise splitting (see Methods) is common in machine learning but leaks information between the train and validation sets when used in transcriptomic tasks.
To avoid this data leakage, we split the dataset at the study level when that information was available.
We found that there is in fact a large degree of performance inflation evident when comparing the sample-split results to the study-split results in the Recount3 multiclass setting (supp. fig. @fig:splitting).
While this supports our decision to use study-level splitting, the relative performance of each model stays the same regardless of data splitting technique.

Another common usage pattern in machine learning is to train models on a general dataset then fine-tune them on a dataset of interest.
To ensure that our results weren't made irrelevant by different behavior in the pretraining context, we examined the performance of the models with and without pretraining (supp. fig @fig:pretrain).
To do so we split the Recount3 data into three sets: pretraining, training, and validation (fig. @fig:workflow bottom), then trained two identically initialized copies of each model.
One copy was trained solely on the training data, while the other was trained on the pretraining data then fine-tuned on the training data.

The pretrained models showed high performance even when trained with small amounts of data from the training set.
However, the nonlinear models did not have a greater performance gain from pretraining than logistic regression, and the balanced accuracy was similar across models.
In fact, all models showed lower performance than when using the full training data, as models forget information from previous runs during fine-tuning [@doi:10/ckpftf].

