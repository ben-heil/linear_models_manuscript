## Results 

### Approach 
We compared the performance of linear and nonlinear models across a number of datasets and on multiple tasks (fig. @fig:workflow top).
Our datasets consisted of gene expression from Recount3 [@doi:10.1186/s13059-021-02533-6] with tissue labels from the recount3 metadata and sex labels from Flynn et al. [@doi:10.1186/s12859-021-04070-2], simulated data, and expression and tissue labels from GTEx[@doi:10.1038/ng.2653].
Before use in model training, we removed scRNA samples, RPKM normalized, and zero-one standardized the data (see Methods for more details).

We split our dataset via fivefold cross-validation to evaluate each model on multiple training and validation sets.
In order to avoid leakage between folds, the studies were placed in their entirety into a fold instead of being split across folds (fig. @fig:workflow bottom).
We then ran the models on increasingly large subsets of the training data to determine how model performance is affected by the amount of training data.

To ensure that artifacts specific to a single data split or model initialization don't drive the signal, we run each of our experiments with three different random seeds.
As a result of these different dimensions of variation, each evaluation we perform reflects 150 trained instances of each model.
The three models we selected were logistic regression, a three layer neural network, and a five layer neural network.
Our three layer and five layer networks were chosen to be representative of a fairly shallow and moderately deep network respectively.
Our logistic regression implementation was implemented and optimized as similarly to the neural nets as possible to allow comparisons unbiased by implementation details.
For a comparison against a scikit-learn [@url:https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html] implementation of logistic regression, see Supplementary Results section TODO.

![
Schematic of the model analysis workflow. We evaluate three models on multiple classification problems in three datasets (top). To do so, we use studywise splitting as a default, but also evaluate the effects of samplewise splitting and using a pretraining dataset.
](./images/workflow.svg "Workflow diagram"){#fig:workflow}


### Linear and nonlinear model comparison
To determine whether linear and nonlinear models performed similarly, we used them for a number of tissue prediction tasks.
First we compared their abilities to differentiate between pairs of tissues (Fig. @fig:recount-binary), and found that their performance was roughly equivalent.
More specifically, given tissue pairs seemed to have maximum accuracy thresholds that models would achieve and then plateau.

In case the accuracy threshold effects were due to the relative easiness of the binary classification task, we selected a harder problem.
Namely, we evaluated the models on their ability to predict which of the 21 most common tissues in the Recount3 dataset a sample belonged to.
In this setting, we found that our five layer network and logistic regression performed roughly the same, while the three layer network had lower accuracy (fig. @fig:recount-multiclass).

![
Comparison of models' binary classification performance before and after removing linear signal
](./images/recount_binary_combined.svg "Recount binary classification before and after signal removal"){#fig:recount-binary}

![
Graph of the Recount3 multiclass classification results. Each point represents the validation set balanced accuracy of a separate trained model. The color of the points and the trend lines shows their corresponding model class, while the dashed line represents the baseline accuracy of random predictions.
](./images/recount_multiclass.svg "Recount multiclass classification"){#fig:recount-multiclass}

### Nonlinear signal in the data
One can imagine a world where all the signal relevant to tissue prediction is linear.
If that were the case, nonlinear models like neural networks would fail to give any advantage in a prediction problem.
To determine whether there is nonlinear signal in our tissue prediction tasks learnable by our neural nets, we used Limma[@doi:10.1093/nar/gkv007] to remove the linear signal associated with each tissue.

When we ran our models on the signal-removed data, we found that while the neural networks manage to perform better than the random classification baseline, the logistic regression models do worse than the baseline (Fig. @fig:recount-binary).
The anticorrelation between the amount of data used and the linear model performance is due to running the signal removal on the full dataset at once.
Because there can be no predictive linear signal in the dataset, any linear model trained to greater than random performance on the training set will necessarily perform worse than random on the rest of the data.
This artifact was selected as the lesser of two evils, as removing signal from the training and validation set poses its own problems (Supp results TODO).

### Simulation experiments
We then simulated simple binary classification tasks to ensure that the results weren't due to an unknown signal in the data.
Our initial simulated dataset consisted of two types of features: half of the features had a linear dividing line between the simulated classes while the other half had a nonlinear dividing line.
After training to classify the simulated dataset, all models were able to effectively predict the simulated classes.
After removing the linear signal from the dataset, nonlinear models were still able to easily predict the correct classes, but logistic regression was no better than random (fig @fig:simulation middle).

To ensure that the high performance of the nonlinear models wasn't due to nonlinear signal induced by the correction method, we generated another simulated dataset consiting solely of features with a linear dividing line between the classes.
As before, all models were able to predict the different classes well.
However, once the linear signal was removed all models had accuracy no better than random guessing, indicating that the signal removal method was not generating nonlinear signal (fig @fig:simulation left).

We also trained the models on a dataset where all features were gaussian noise as a negative control.
As expected, the models all performed at baseline accuracy both before and after the signal removal process (fig. @fig:simulation right).

![
Performance of models in binary classification of simulated data before and after signal removal
](./images/simulated_data_combined.svg ){#fig:simulation}

### GTEx validation
To validate our findings on a separate real dataset, we selected the expression data from GTEx [@doi:10.1038/ng.2653].
Because it was generated by fewer labs with more consistent experimental design across samples, it is a less heterogeneous dataset than the Recount3 compendium.
We trained our models to do binary classification on pairs of the five most common tissues in the dataset, then performed multiclass classification on all 31 tissues present in the dataset.
Likely due to the cleaner nature of the GTEx data, all models were able to perform perfectly on the binary classification tasks (Fig. @fig:gtex bottom)
The harder multitask classification problem showed logistic regression outperforming the five layer neural network, which in turn outperformed the three layer net (fig. @fig:gtex top).

The linear signal removal results on the binary classification problems were consistent with those from the Recount3 compendium.
The neural networks performed less well in the low-data regime, indicating an increase in the difficulty of the problem, and the logistic regression implementation performed no better than random (Fig. @fig:gtex bottom).
Similarly, the multiclass problem had the logistic regression model performing poorly, while the nonlinear models had performance that increased with an increase in data while remaining worse than before the linear signal was removed (Fig. @fig:gtex top).

![
Performance of model on GTEx classification problems. The top figures show the difference in training models in the multiclass setting with and without signal removal, while the bottom figures show binary classification with and without signal removal.
](./images/gtex_combined.svg ){#fig:gtex}

### Sex prediction validation
To rule out the possibility that our findings were specific to tissue prediction tasks, we examined models' ability to predict metadata-derived sex (Fig. @fig:sex-prediction).
We used the same experimental setup as in our other Recount3 binary prediction tasks to train the models, but rather than using tissue labels we used metadata-derived sex labels.
In this setting we found that, at least in the 5000-15000 sample range, the nonlinear models outperformed logistic regression.
This result demonstrates that despite the compelling accuracy of linear models, there are still problem settings where nonlinear models perform better.

![
Metadata sex prediction
](./images/sex_prediction.svg ){#fig:sex-prediction}

### Pretraining 
A common usage pattern in machine learning is to train models on a general dataset then fine-tune them on a dataset of interest.
To ensure that our results weren't made irrelevant by different behavior in the pretraining context, we examined the performance of the models with and without pretraining (Supp. fig TODO).
We split our data into three sets: pretraining, training, and validation (Fig. @fig:workflow bottom), then trained two identically initialized copies of each model.
One copy was trained solely on the training data, while the other was trained on the pretraining data then fine-tuned on the training data.

The pretrained models showed high performance even when trained with small amounts of data from the training set.
However, the nonlinear models did not have a greater performance gain from pretraining than logistic regression, and the balanced accuracy was similar across models.
In fact, all models showed lower performance than when using the full training data, as models forget information from previous runs during fine-tuning [@doi:10/ckpftf].

### Sample splitting
We considered it possible that our results were an artifact of our method of dataset splitting, and set out to test it.
There is a common method of data splitting we refer to as samplewise splitting (see Methods) that leaks information between the train and validation sets when used in transcriptomic tasks.
To avoid this data leakage, we split the dataset at the study level in our Recount3 analyses.
We found that there is in fact a large degree of performance inflation evident when comparing the sample-split results to the study-split results in the Recount3 multiclass setting (Supp Fig. @fig:splitting).
While this supports our decision to use study-level splitting, the relative performance of each model stays the same regardless of data splitting technique.
