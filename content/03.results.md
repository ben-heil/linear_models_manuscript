## Results 

### Approach 
TODO workflow diagram here

Our dataset is composed of gene expression from recount3 [CITE], with tissue labels from the recount3 metadata and sex labels from Flynn et al. [CITE].
Before use in model training, we removed scRNA samples, TPM normalized, and zero-one standardized the data (see Methods for more details).

We split our dataset via fivefold cross-validation to evaluate each model on multiple training and testing sets.
In order to avoid leakage between folds, the studies were placed in their entirety into a fold instead of being split across folds.
We also ran the models on increasingly large subsets of the training data to determine how model performance is affected by the amount of training data.

To ensure that artifacts specific to a single data split or model initialization don't drive the signal, we run each of our experiments with three different random seeds.
As a results of these different dimensions of variation, each evaluation we perform reflects 150 trained instances of each model.

TODO why did we use the models we did


### Linear and nonlinear model comparison
To determine whether linear or nonlinear models performed similarly, we set them to a number of tissue prediction tasks.
First we compared their abilities to differentiate between pairs of tissues (Fig. @fig:binary-classification).
We found that their performance was roughly equivalent.
More specifically, given tissue pairs seem to have maximum accuracy thresholds that models achieve and then stagnate.

In case the accuracy threshold effects were due to the relative easiness of the binary classification task, we selected a harder problem.
Namely, we evaluated the models on their ability to predict which of the 21 most common tissues in the dataset a sample belonged to.
For this task, the scikit-learn implementation of logistic regression outperformed the other models, with our PyTorch logistic regression implementation and five layer neural network having similar performance and the three layer neural network performing the worst (Fig. @fig:multiclass).
We suspect that this is due to scikit-learn linear regression's optimization method outperforming the stochastic gradient descent used by our other models.

### Nonlinear signal in the data
One can imagine a world where all the signal relevant to tissue prediction is linear.
If that were the case, nonlinear models like neural networks would fail to give any advantage in a prediction problem.
To determine whether there is nonlinear signal in our tissue prediction tasks that is learnable by our neural nets, we used Limma[@doi:10.1093/nar/gkv007] to remove the linear signal associated with each tissue.
When we ran our models on the signal-removed data, we found that while the neural networks manage to perform better than the random classification baseline, the logistic regression models do worse than the baseline (Fig. @fig:binary-signal-removed).
TODO describe curves
We hypothesize that this underperformance is caused by the linear models fitting to noise that is spuriously correlated with the tissue labels in the training set.

### Simulation experiments
We then simulated simple binary classification tasks to ensure that the results weren't due to an unknown attribute of the data.
Our initial simulated dataset consisted of fifty percent features with a linear dividing line between the simulated classes and fifty percent features with a nonlinear dividing line.
Upon training models to classify the simulated dataset, all models were able to effectively predict the simulated classes.
After removing the linear signal from the dataset, nonlinear models were still able to easily predict the correct classes, but logistic regression was no better than random.

To ensure that the high performance of the nonlinear models wasn't due to nonlinear signal induced by the correction method, we generated another simulated dataset with solely linear features.
As before, all models were able to predict the different classes well.
However, once the linear signal was removed all models had accuracy no better than random guessing, indicating that the signal removal method was not generating nonlinear signal.

We also trained the models no a dataset of gaussian noise as a negative control.
As expected, the models all performed at baseline accuracy both before and after the signal removal process.

TODO should I talk about the worse-than-random accuracy of the linear model(s) and how it is caused by full dataset be correction?

### Pretraining 
A common usage pattern in machine learning is to train models on a general dataset then fine-tune them on a dataset of interest[CITE?].
To ensure that our results weren't made irrelevant by different behavior in this pretraining contex, we examined the performance of the models with and without pretraining.
We split our data into three sets: pretraining, training, and validation, then trained two identically initialized copies of each model.
One copy was trained solely on the training data, while the other was trained on the pretraining data then fine-tuned on the training data.

The pretrained models showed high performance even when trained with small amounts of data from the training set.
However, the nonlinear models did not have a greater performance gain from pretraining than logistic regression, and the balanced accuracy was similar across models.

### Sample splitting
There is a common method of data splitting we refer to as samplewise splitting (see methods section X) that leaks information between the train and test sets when used in transcriptomic tasks.
To avoid this data leakage, we split the dataset at the study level.
We considered it possible that our results were an artifact of our method of dataset splitting, and set out to test it.
To do so, we ran our multi-tissue analysis using sample splitting and found X (TODO).

### GTEx validation
To validate our findings on a second real dataset, we selected the expression data from GTEx[CITE].
Because it was generated by fewer labs with more consistent experimental design across samples, it is a less heterogeneous dataset than the Recount compendium.
We trained our models to do binary classification on pairs of the five most common tissues in the dataset, then performed multiclass classification on all 31 tissues present in the dataset.
Likely due to the cleaner nature of the GTEx data, all models were able to perform perfectly on the binary classification tasks.
The harder multitask classification problem showed logistic regression outperforming the five layer neural network, which in turn outperformed the three layer net.

The linear signal removal results on the binary classification problems were consistent with those from the recount compendium.
The neural networks performed less well in the low-data regime, indicating an increase in the difficulty of the problem, and the logistic regression implementation performed no better than random.
Similarly, the multiclass problem had the logistic regression model performing poorly, while the nonlinear models had performance that increased with an increase in data while remaining worse than before the linear signal was removed.

### Sex prediction validation
To rule out the possibility that our findings were specific to tissue prediction tasks, we examined models' ability to predict metadata-derived sex (Fig. @fig:sex-prediction).
We used the same experimental setup as in our other binary prediction tasks to train the models, but rather than using two tissues as labels, we used sex.
Once again the task showed similar performance between logistic regression and neural nets.
TODO sentence describing curve

### Data exploration
To determine whether our results were driven by an artifact in the data, we performed exploratory data analysis.
First, we looked for whether anything stood out when comparing per-sample performance between models.
Upon doing so, we found that TODO describe after running on all-tissue
When looking at the results, we noticed that some samples were consistently misclassified across models. 
We suspected it might be due to label imbalance, but a confusion matrix showed that not to be the case.
We examined the metadata for attributes that might be correlated with sample prediction hardness, and found that these samples tend to have a lower read quality than other samples.
