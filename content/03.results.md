## Results 

### Approach (or Constructing classifiers for expression data)
To compare linear and nonlinear models, we first needed a consistent way to train them and determine their performance.
Because model performance is dependent on both model initialization and training data, we decided to use several instances of both.
On the data side, the we split the data via fivefold cross-validation to evaluate each model on multiple training and testing sets.
In order to avoid leakage between folds, the studies were placed in their entirety into a fold instead of being split across folds.
We also run the models on increasingly large subsets of the training data to determine how model performance is affected by the amount of training data.
To ensure that artifacts specific to a single data split or model initialization don't drive the signal, we run each of our experiments with three different random seeds.
As a results of these different dimensions of variation, each evaluation we perform reflects 150 trained instances of each model.

- Maybe make a workflow diagram?

### Linear and nonlinear models perform equivalently on metadata prediction tasks
To determine whether linear or nonlinear models performed similarly, we set them to a number of tissue prediction tasks.
First we compared their abilities to differentiate between pairs of tissues (Fig. @fig:binary-classification).
We found that their performance was roughly equivalent.
More specifically, given tissue pairs seem to have maximum accuracy thresholds that models achieve and then stagnate.

In case the accuracy threshold effects were due to the relative easiness of the binary classification task, we selected a harder problem.
Namely, we evaluated the models on their ability to predict which of the 21 most common tissues in the dataset a sample belonged to.
For this task, the scikit-learn implementation of logistic regression outperformed the other models, with our Pytorch logistic regression implementation and five layer neural network having similar performance and the three layer neural network performing the worst (Fig. @fig:multiclass).
We suspect that this is due to scikit-learn linear regression's default hyperparameters outperforming our hyperparameters for logistic regression.

### This isn't because there is only linear signal in the data
One can imagine a world where all the signal relevant to tissue prediction is linear.
If that were the case, nonlinear models like neural networks would fail to give any advantage in a prediction problem.
To determine whether there is nonlinear signal in our tissue prediction tasks that is learnable by our neural nets, we used Limma [CITE] to remove the linear signal associated with each tissue.
When we ran our models on the signal-removed data, we found that while the neural networks manage to perform better than the random classification baseline, the logistic regression models do worse than the baseline (Fig. @fig:binary-signal-removed).
TODO describe curves
We hypothesize that this underperformance is caused by the linear models fitting to noise that is spuriously correlated with the tissue labels in the training set.

### This isn't because tissue prediction is special
To rule out the possibility that our findings were specific to tissue prediction tasks, we examined models' ability to predict metadata-derived sex (Fig. @fig:sex-prediction).
We used the same experimental setup as in our other binary prediction tasks to train the models, but rather than using two tissues as labels, we used sex.
Once again the task showed similar performance between logistic regression and neural nets.
TODO sentence describing curve

### This is still true when there is data leakage as in common data splitting formats
There is a common method of data splitting we refer to as samplewise splitting (see methods section X) that leaks informatiion between the train and test sets when used in transcriptomic tasks.
To avoid this data leakage, we split the dataset at the study level.
We considered it possible that our results were an artifact of our method of dataset splitting, and set out to test it.
To do so, we ran our multi-tissue analysis using sample splitting and found X.

- Maybe also make supplementary figures of binary classification with sample splitting?

### Pretraining doesn't save things (should include a section on transfer learning?)

- Pretraining plots
