## Introduction

Deep learning is rapidly proving to be a useful tool in the computational biology toolbox.
From X, to Y, to Z, [CITE, CITE, CITE], older, less complex models are being superceded by neural networks.

One area where deep learning might prove to be useful is in making predictions from transcriptomic data.
Gene expression-based models are already being used to subtype cancer [CITE PAM], predict transplant rejections [CITE https://doi.org/10.1161/CIRCULATIONAHA.116.022907], and uncover biases in public data [CITE Flynn].
It would make sense for more powerful models might allow better transcriptomic predictions, advancing such fields.

In fact, gene expression seems like an excellent place to apply DL models.
There are millions of publicly available gene expression samples in the Sequence Read Archive [CITE], several projects that uniformly preprocess the raw data [CITE refine.bio, recount3, ARCHS4], and an array of biologically interesting problems.
As a result, there are many studies applying deep learning to transcriptomics [CITE some].

While some studies compare their deep learning model of choice to baseline models, there have been few systematically comparing simple models with deep learning in transcriptomics [maybe cite https://www.sciencedirect.com/science/article/abs/pii/S0895435618310813].
The most relevant is a paper Smith et al. [CITE] which states that classical machine learning approaches perform better than deep representation learning algorithms.
However, their focus was more on whether representation learning was helpful than on whether linear methods were equivalent to neural nets (TODO double check/reread paper to ensure statement is accurate).

To determine whether deep learning models are actually helpful, we created a benchmark for measuring model performance in predicting what tissue a gene expression sample came from.
The benchmark uses XXX,XXX rna-seq samples from recount3[CITE], manually curated tissue labels, and four different model implementations.
TODO elaborate (?)

We show that while deep neural networks are able to learn nonlinear signals relevant to predicting tasks, these signals aren't strong enough to make a different in transcriptomics.
As a result, we find logistic regression models perform just as well as neural networks in predicting tissue or metadata-derived sex.
Finally, we show that these results aren't due to any obvious signals in the training data, such as imbalanced classes or quality control issues.


- These results suggests [too much complexity, data too small, data too noisy]

- Going forward, scientists should be aware of this when selecting models? 
