## Introduction

Transcriptomic data contains a wealth of information about a person's biology, so predicting phenotypes from RNA-seq data is a promising field of research.
Gene expression-based models are already being used to subtype cancer [@doi:10.1200/JCO.2008.18.1370], predict transplant rejections [@doi:10.1161/CIRCULATIONAHA.116.022907], and uncover biases in public data [@pmc:PMC8011224].
In fact, both the capability of machine learning models [@arxiv:2202.05924] and the amount of transcriptomic data available [@doi:10.1038/s41467-018-03751-6; @doi:10.1093/database/baaa073] are increasing rapidly.
It makes sense, then, that neural networks are frequently being used in the transcriptomic prediction space [@doi:10.1038/s41598-019-52937-5; @doi:10.1093/gigascience/giab064; @doi:10.1371/journal.pcbi.1009433].

However, there are two conflicting ideas in the literature regarding the utility of nonlinear models.
One theory is based on the prior biological understanding: the paths linking gene expression to phenotypes are complex[@doi:10.1016/j.semcdb.2011.12.004, @doi:10.1371/journal.pone.0153295], so nonlinear models like neural networks should have a better capacity to learn that complexity.
Accordingly, many have used nonlinear models to learn representations useful for making predictions of phenotypes from gene expression [@doi:10.1128/mSystems.00025-15; 10.1371/journal.pcbi.1009433; @doi:10.1016/j.cmpb.2018.10.004; @doi:10.1186/s12859-017-1984-2]
Unlike purely linear models like logistic regression, nonlinear models are able to learn more sophisticated representations of the relationships between expression and phenotype.

The other theory flies in the face of that hypothesis: when using expression to make predictions about phenotypes, linear models seems to do as well or better than nonlinear ones in many cases[@doi:10.1186/s12859-020-3427-8].
While papers of this sort are harder to come by — scientists don't tend to write papers about how their deep learning model was worse than logistic regression — other complex biological problems have also seen linear models prove equivalent to nonlinear ones[@doi:10.1016/j.jclinepi.2019.02.004].

In this paper we demonstrate that both theories are true: there is nonlinear signal relating the phenotypes to genotypes, but it doesn't always lead nonlinear models to provide better prediction accuracy.
We begin by constructing a system of binary and multi-class classification problems on the Recount3 compendium[@doi:10.1186/s13059-021-02533-6] that allows us to show that linear and nonlinear models have similar accuracy on several (but not all) prediction tasks.
We then remove the linear signals relating the phenotype to gene expression and show that there is in fact nonlinear signal in the data even when the linear models outperform the nonlinear ones.
Finally, we validate the results by testing the same problems on a dataset from GTEx[@doi:doi.org/10.1038/ng.2653], running controls on simulated data, and examining different problem formulations such as samplewise splitting and pretraining.

In reconciling these two obstensibly conflicting theories, we assist future scientists by showing the importance of trying a linear baseline model before developing a complex nonlinear approach.
While nonlinear models may outperform simpler models at the limit of infinite data, they don't necessarily do so even when trained on the largest datasets publicly available today.
