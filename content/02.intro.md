## Introduction

Making predictions from transcriptomic data has many potential uses.
Gene expression-based models are already being used to subtype cancer [@doi:10.1200/JCO.2008.18.1370], predict transplant rejections [@doi:10.1161/CIRCULATIONAHA.116.022907], and uncover biases in public data [@pmc:PMC8011224].
It would make sense for more powerful models might allow better transcriptomic predictions, advancing such fields.

However, there are two conflicting theories in the literature regarding the utility of such models.
One theory states the obvious: the paths linking gene expression to phenotypes are nonlinear[CITE], with the corrolary that nonlinear models are better able to learn that complexity [CITE ADAGE].
The other theory flies in the face of that obvious truth: when using expression to make predictions about phenotypes, linear models seems to do as well or better than nonlinear ones [CITE].

In this paper we demonstrate that both theories are true.
We begin by constructing a system of binary and multi-class classification problems on the Recount3 compendium[CITE] that allows us to show that linear and nonlinear models have similar accuracy on several tissue prediction tasks.
We then remove the linear signals relating tissue to gene expression and show that there is in fact nonlinear signal in the data.
Finally, we validate the results by testing the same problems on a dataset from GTEx[CITE], running controls on simulated data, and probing potential failure cases.

In reconciling these two obstensibly conflicting theories, we assist future scientists by showing the added complexity of designing and training large neural networks is unnecessary in many cases.
While such models may outperform simpler models at the limit of infinite data, they don't do so even when trained on the largest datasets publicly available today.
