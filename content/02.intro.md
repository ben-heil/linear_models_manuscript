## Introduction

Deep learning is rapidly proving to be a useful tool in the computational biology toolbox.
From unraveling DNA sequences' effects on gene regulation, to predicting transcription factor binding, to analyzing cell images, [@pmc:PMC8490152; @doi:10.1371/journal.pcbi.1005403; @pubmed:31133758], classical machine learning models are being superceded by neural networks.

One area where deep learning might prove to be useful is in making predictions from transcriptomic data.
Gene expression-based models are already being used to subtype cancer [@doi:10.1200/JCO.2008.18.1370], predict transplant rejections [@doi:10.1161/CIRCULATIONAHA.116.022907], and uncover biases in public data [@pmc:PMC8011224].
It would make sense for more powerful models might allow better transcriptomic predictions, advancing such fields.

In fact, gene expression seems like an excellent place to apply DL models.
There are millions of publicly available gene expression samples in the Sequence Read Archive [@doi:10.1093/nar/gkq1019], several projects that uniformly preprocess the raw data [@refinebio; @pmc:PMC8628444; @pmc:PMC5893633], and an array of biologically interesting problems.
As a result, there many studies apply deep learning to transcriptomics [CITE some].

While some studies compare their deep learning model of choice to baseline models, there have been few systematically comparing simple models with deep learning in transcriptomics [@doi:10.1016/j.jclinepi.2019.02.004; @pmc:PMC7085143] 
The most relevant is a paper Smith et al. which states that classical machine learning approaches perform better than deep representation learning algorithms.
However, their focus was more on whether representation learning was helpful than on whether linear methods were equivalent to neural nets (TODO double check/reread paper to ensure statement is accurate).

To determine whether deep learning models are actually helpful, we created a benchmark for measuring model performance in predicting what tissue a gene expression sample came from.
The benchmark uses XXX,XXX rna-seq samples from recount3 [@pmc:PMC86284], manually curated tissue labels, and four different model implementations.
TODO elaborate (?)

We demonstrate that while deep neural networks are able to learn nonlinear signals relevant to predicting tasks, these signals aren't strong enough to make a different in transcriptomics.
As a result, we find logistic regression models perform just as well as neural networks in predicting tissue or metadata-derived sex.
Finally, we show that these results aren't due to any obvious signals in the training data, such as imbalanced classes or quality control issues.


- These results suggest [too much complexity, data too small, data too noisy]

- Going forward, scientists should be aware of this when selecting models? 
