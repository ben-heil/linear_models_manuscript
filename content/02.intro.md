## Introduction

Transcriptomic data contains a wealth of information about biology.
Gene expression-based models are already being used for subtyping cancer [@doi:10.1200/JCO.2008.18.1370], predicting transplant rejections [@doi:10.1161/CIRCULATIONAHA.116.022907], and uncovering biases in public data [@pmc:PMC8011224].
In fact, both the capability of machine learning models [@arxiv:2202.05924] and the amount of transcriptomic data available [@doi:10.1038/s41467-018-03751-6; @doi:10.1093/database/baaa073] are increasing rapidly.
It makes sense, then, that neural networks are frequently being used in the transcriptomic prediction space [@doi:10.1038/s41598-019-52937-5; @doi:10.1093/gigascience/giab064; @doi:10.1371/journal.pcbi.1009433].

However, there are two conflicting ideas in the literature regarding the utility of nonlinear models.
One theory draws on prior biological understanding: the paths linking gene expression to phenotypes are complex [@doi:10.1016/j.semcdb.2011.12.004; @doi:10.1371/journal.pone.0153295], and nonlinear models like neural networks should be more capable of learning that complexity.
Unlike purely linear models such as logistic regression, nonlinear models should learn more sophisticated representations of the relationships between expression and phenotype.
Accordingly, many have used nonlinear models to learn representations useful for making predictions of phenotypes from gene expression [@doi:10.1128/mSystems.00025-15; @doi:10.1016/j.cmpb.2018.10.004; @doi:10.1186/s12859-017-1984-2].

The other theory disagrees with the first hypothesis.
When using expression to make predictions about phenotypes, linear models seem to do as well as or better than nonlinear ones in many cases [@doi:10.1186/s12859-020-3427-8].
While papers of this sort are harder to come by — scientists do not tend to write papers about how their deep learning model was worse than logistic regression — other complex biological problems have also seen linear models prove equivalent to nonlinear ones [@doi:10.1016/j.jclinepi.2019.02.004].

We demonstrate that both theories have merit.
There is nonlinear signal relating the phenotypes to genotypes, but the signal does not always lead nonlinear models to provide better predictive accuracy.
We construct a system of binary and multi-class classification problems on the GTEx and Recount3 compendia [@doi:10.1038/ng.2653;@doi:10.1186/s13059-021-02533-6] that shows linear and nonlinear models have similar accuracy on several prediction tasks.
We then remove the linear signals relating the phenotype to gene expression and show that there is nonlinear signal in the data even when the linear models outperform the nonlinear ones.
Finally, we validate the results by testing our models on a sex-metadata prediction problem, running controls on simulated data, and examining different problem formulations such as samplewise splitting and pretraining.

In reconciling these two ostensibly conflicting theories, we assist future scientists by showing the importance of trying a linear baseline model before developing a complex nonlinear approach.
While nonlinear models may outperform simpler models at the limit of infinite data, they do not necessarily do so even when trained on the largest datasets publicly available today.
