## Conclusion

We performed a series of analyses determining the relative performance of linear and nonlinear models in multiple domains.
We found that, consistent with previous papers [@doi:10.1186/s12859-020-3427-8; @doi:10.1016/j.jclinepi.2019.02.004], linear and nonlinear models performed roughly equivalently in a number of tasks.
That is to say that there are some tasks where linear models perform better, some tasks where nonlinear models have better performance, and some tasks where both model types are equivalent.

When we removed all linear signal in the data we found that residual nonlinear signal remained.
Not only was this true in simulated data, but also held in GTEx and Recount3 data in a number of problems.
These results also held in slightly altered problem settings, such as using a pretraining dataset before the training dataset and using samplewise data splitting instead of studywise splitting.
This consistant presence of nonlinear signal demonstrated that the similarity in performance across model types was not due to our problem domains having solely linear signals.

Given that there is nonlinear signal present in our problem domains, why doesn't that signal allow nonlinear models to make better predictions?
We believe that the nonlinear signal is either redundant with the linear signal, or unreliable enough that nonlinear models choose to learn the linear signal instead.
Determining which of these hypotheses (if either) is true is an interesting avenue for future research.

One limitation of our study is that the results likely do not hold in an infinite data setting.
Deep learning models have been shown to solve hard problems in biology and tend to greatly outperform linear models when given enough data.
However, we do not yet live in a world with huge amounts of gene expression data and accompanying uniform metadata.
Our results are generated on some of the largest labeled expression datasets in existence (Recount3 and GTEx), but our tens of thousands of samples are far from the millions or billions used in deep learning research.

We are also unable to make claims about all problem domains.
There are many potential transcriptomic prediction tasks, and many datasets to perform them on.
While we show that nonlinear signal is not always helpful in tissue or sex prediction, and others have shown the same for various disease prediction tasks, there may be problems where nonlinear signal is more important.

Ultimately, our results show that the existence of task-relevant nonlinear signal in the data does not necessarily lead nonlinear models to outperform linear ones.
Additionally we demonstrate that while there are problems where complicated models are useful, scientists making predictions from expression data should always include simple linear baseline models to determine whether more complex models are warranted.

