## Conclusion

In this paper, we performed a series of analyses determining the relative performance of linear and nonlinear models in multiple domains.
We found that, consistent with previous papers[@doi:10.1186/s12859-020-3427-8; @doi:10.1016/j.jclinepi.2019.02.004], linear and nonlinear models performed roughly equivalently in a number of tasks.
That is to say that there are some tasks where linear models perform better, some tasks where nonlinear models have better performance, and some tasks where both model types are equivalent.

To determine what led to the performance of the two model classes, we removed all linear signal in the data and found that even in situations where both model types had the same performance there was residual signal that only our nonlinear models were capable of learning.
This implies that the results that we observed were not driven by a lack of nonlinear signal.
We then simulated data to ensure that the signal removal method was not inducing nonlinear signal.
We continued by showing that these results held in slightly altered problem settings, such as using a pretraining dataset before the training dataset and using samplewise data splitting instead of studywise splitting.
Finally, we validated our results on different datasets and domains by running the same analyses on GTEx data and predicting sex labels from expression.

We were able to show that there is both linear and nonlinear signal in our datasets, but that the existence of nonlinear signal does not necessarily lead nonlinear models to make higher-accuracy predictions.
Given that there is nonlinear signal that relates expression to tissue types, why is it that such signal doesn't allow models to make better predictions?
We believe that it is because the nonlinear signal is either redundant with the linear signal, or unreliable enough that nonlinear models choose to learn the linear signal instead.

One limitation of our study is that the results likely do not hold in an infinite data setting.
Deep learning models have been shown to solve hard problems in biology and tend to greatly outperform linear models when given enough data.
However, we do not yet live in a world with huge amounts of gene expression data and accompanying uniform metadata.
Our results are generated on some of the largest labeled expression datasets in existence (Recount3 and GTEx), but our tens of thousands of samples are far from the millions or billions used in deep learning research.

We are also unable to make claims about all problem domains.
There are many potential transcriptomic prediction tasks, and many datasets to perform them on.
While we show that nonlinear signal is not always helpful in tissue prediction, and others have shown the same for various disease prediction tasks, there are also problems such as sex metadata prediction where the nonlinear signal seems to be important.

Ultimately, our results show that the existence of task-relevant nonlinear signal in the data does not necessarily lead nonlinear models to outperform linear ones.
Determining what causes this disconnect is an exciting avenue of future research.
Additionally we demonstrate that while there are problems where complicated models are useful, scientists making predictions from expression data should always include simple linear baseline models to determine whether more complex models are warranted.

