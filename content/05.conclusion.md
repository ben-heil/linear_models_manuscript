
- Results sketch
In this paper, we performed a series of analyses determining the relative performance of linear and nonlinear models in multiple domains.
We found that, consistent with previous papers[CITE], linear and nonlinear models performed roughly equivalently in a number of tasks.
To determine what led to the similar performance of the two model classes, we removed all linear signal in the data and found that there was residual signal that only our nonlinear models were capable of learning.
We then simulated data to ensure that the signal removal method wasn't not inducing nonlinear signal that didn't already exist.
We continued by showing that these results held in slightly altered problem settings, such as using a pretraining dataset before the training dataset and using samplewise data splitting instead of studywise splitting.
Finally, we validated our results on different datasets and domains.
Namely, we ran the same analyses on GTEx data, predicted sex labels from expression, and analyzed error patterns from our models to see whether there was a systematic factor causing consistent errors across models.


- We resolve the conflict in the intro by showing X

We were able to show that there is both linear and nonlinear signal in the data, but that the existence of nonlinear data does not cause nonlinear models to make higher-accuracy predictions.
Given that there is nonlinear signal that relates expression to tissue types, why is it that such signal doesn't allow models to make better predictions?
We believe that it is because the nonlinear signal is either redundant with the linear signal, or unreliable enough that nonlinear models choose to learn the linear signal instead.
What causes that redundancy or unreliability is an exciting avenue for future research.

- These results are more comprehensive than other papers

One limitation of our study is that the results likely don't hold in an infinite data setting.
Deep learning models have  been shown to solve hard problems in biology [CITE alphafold; basenji?; @doi:10/bxwn] when given enough data.
However, we don't yet live in a world with huge amounts of gene expression data with accompanying uniform metadata.
Our results are generated on some of the largest expression datasets in existence (Recount3 and GTEx), but our tens of thousands of samples are far from the millions or billions used in deep learning research [@doi:10/cvc7xp; @arxiv:2101.00027; @arxiv:2103.01913]

- Limitation - can't make a claim about all problem domains, just that this is something that occurs frequently

- Our results indicate that future scientists should try simple models first


