## Methods
### Recount3 tissues used
The tissues used from Recount3 were blood, breast, stem cell, cervix, brain, kidney, umbilical cord, lung, epithelium, prostate, liver, heart, skin, colon, bone marrow, muscle, tonsil, blood vessel, spinal cord, testis, and placenta.

### Data exploration
To determine whether our results were driven by an artifact in the data, we performed exploratory data analysis.
First, we looked for whether anything stood out when comparing per-sample performance between models.
Upon doing so, we found that TODO describe after running on all-tissue
When looking at the results, we noticed that some samples were consistently misclassified across models. 
We suspected it might be due to label imbalance, but a confusion matrix showed that not to be the case.
We examined the metadata for attributes that might be correlated with sample prediction hardness, and found that these samples tend to have a lower read quality than other samples.

## Results 

### Signal removal
As mentioned in the main results section, training a more accurate linear model on signal-removed training data leads to a less accurate model on the validation data due to removing signal on the entire dataset at once.
However, the alternative can lead to even worse artifacts, as seen in fig. sup. @fig:split-signal-correction, where the the linear and nonlinear models have random average performance via different methods.
We suspect the swings in model performance in the nonlinear data are due to colinearity in the features.
That is to say that given the option of a number of possible corrections that can be made to remove the linear signal in the data, there is no guarantee that the same one is selected in the train and validation sets.
For that reason, it's possible to end up with results where the model performance varies wildly in different runs of the signal removal method based on the input data.

![                                                                                                                                                                                                          
Sex prediction results when removing signal from training fold and validation fold separately.
](./images/sex_prediction_split_signal.svg "Sex prediction split signal"){#fig:split-signal-correction} 

### Scikit-learn logistic regression
The Pytorch logistic regression implementation we used was designed to be as close to the neural network implementations as possible to ensure the models were comparable.
Accordingly, we were optimizing for similarity of implementation instead of maximal performance.
Scikit-learn, on the other hand, optimizes their models to have the best out-of-box performance they can manage.
To understand the magnitude of the difference, we compared the sklearn logistic regression model to our pytorch models.
We found that it generally outperformed the other models (supp. fig. @fig:sklearn).

![                                                                                                                                                                                                          
TODO description, build figure
](./images/sklearn.svg "Sklearn comparison"){#fig:sklearn} 
