## Methods

### Data

#### Recount3
Our first dataset consisted of bulk RNA-seq data downloaded from the recount3 compendium [@pmc:PMC86284] on TODO date.
Before filtering, the dataset contained 317,258 samples, each containing 63,856 genes.

To filter out single-cell data, we removed all samples with a sparsity greater than 75 percent.
We also removed all samples marked 'scrna-seq' by recount3's pattern matching method (stored in the metadata as 'recount_pred.pattern.predict.type').

To ensure the samples were comparable, we converted the data to transcripts per million using gene lengths from BioMart [@pmc:PMC2649164].
To ensure the genes' magnitudes were comparable, we performed standardization to scale each gene's range from zero to one.
We kept the 5,000 most variable genes within the dataset.

Samples were labeled with their corresponding tissues using the 'recount_pred.curated.tissue' field in the recount3 metadata.
These labels were based on manual curation by the Recount3 authors.
A total of 20324 samples in the dataset had corresponding tissue labels.

Samples were also labeled with their corresponding sex using labels from Flynn et al. [@pmc:PMC8011224].
These labels were derived using pattern matching on metadata from the European Nucleotide Archive [@pmc:PMC3013801].
A total of 23,525 samples in our dataset had sex labels.

#### GTEx 
We downloaded 17,382 TPM-normalized samples of bulk RNA-seq expression data from version 8 of GTEx to validate our results.
We then zero-one standardized the data and kept the 5000 most variable genes.
The tissue labels we used for the GTEx dataset were derived from the 'SMTS' column of the sample metadata file.

#### Simulated data
We generated three simulated datasets to ensure the signal removal process was working as expected.
The first dataset contained 1000 samples of 5000 features corresponding to two classes. 
2500 of those features contained linear signal.
That is to say that the feature values corresponding to one class were drawn from a standard normal distribution, while the feature values corresponding to the other were drawn from a Gaussian with a mean of 6 and unit variance.

The nonlinear features were generated similarly.
The values for the nonlinear features were drawn from a standard normal distribution for one class, while the second class had values drawn from either a mean 6 or mean -6 gaussian with equal probability.
These features are referred to as "nonlinear" because a linear classifier is unable to draw the two dividing lines necessary to correctly classify such data.

The second dataset was similar to the first dataset, but it consisted solely of 2500 linear features.
The final dataset consisted solely of values drawn from a standard normal distribution regardless of class label.

### Model architectures
We use three representative models to demonstrate the performance profiles of different model classes.
Each model was a implemented in Pytorch[@arxiv:1912.01703], used the same optimizer, and was trained for the same max number of epochs.

The nonlinear models were fully connected neural networks.
The first was a three layer network with hidden layers of size 2500 and 1250.
Our second was a five layer network, with hidden layers of size 2500, 2500, 2500, and 1250.

The final model was an implementation of logistic regression, a linear model.
It was designed to be trained as similarly to the neural nets as possible to allow for a fair comparison.

### Model training
#### Optimization
Our models minimized the cross-entropy loss using an Adam [@arxiv:1412.6980] optimizer on minibatches of data.
They also used inverse frequency weighting to avoid giving more weight to more common classes.

#### Regularization
The models used early stopping and gradient clipping to regularize their training.
Both neural nets used dropout[@https://jmlr.org/papers/v15/srivastava14a.html] with a probability of 0.5, and ReLU nonlinearities[@https://dl.acm.org/doi/10.5555/3104322.3104425].
The deeper network used batch normalization[@https://proceedings.mlr.press/v37/ioffe15.html] to mitigate the vanishing gradient problem.

#### Hyperparameters
The hyperparameters for each model can be found in their corresponding config file at https://github.com/greenelab/saged/tree/master/model_configs/supervised.

#### Determinism
Model trainining was set to be deterministic by setting the Python, NumPy, and PyTorch random seeds for each run, as well as setting the PyTorch backends to deterministic and disabling the benchmark mode.

#### Logging
Model training progress was tracked and recorded using Neptune [CITE].

#### Signal removal
We used Limma[@doi:10.1093/nar/gkv007] to remove linear signal associated with tissues in the data.
More precisely, we ran the 'removeBatchEffect' funcion from Limma on the full dataset, using the tissue labels as batch labels.

#### Model Evaluation
In our analyses we use five-fold cross-validation with two types of data splitting.
The first type is samplewise splitting.
In the samplewise paradigm, gene expression samples are split into cross-validation folds at random without respect to which studies they belong to.
In the studywise paradigm, entire studies are assigned to cross-validation folds.

While samplewise splitting is common in the machine learning and computational biology literature, it is ill-suited to gene expression data.
There are study-specific signals in the data, and having samples from the same study in the training and validation sets causes information leakage [CITE? https://www.nature.com/articles/s41576-021-00434-9 ?].
As a result, samplewise splitting inflates the estimated performance of the models.
Studywise splitting avoids leakage by ensuring all the study-specific signals stay within either the training or the validation sets.

We use studywise splitting for the results in the main section of the manuscript, but have added the samplewise results to the supplement to show that the results are not an artifact of data splitting.

#### Hardware
All analyses were performed on an Ubuntu 18.04 machine with 64 GB of RAM.
The CPU used was an AMD Ryzen 7 3800xt processor with 16 cores, and the GPU used was an Nvidia RTX 3090.
The pipeline can be run on a computer with lower specs, but would have to run fewer elements in parallel.
From initiating data download to finishing all analyses and generating all figures, the full Snakemake [@doi:10.1093/bioinformatics/bts480] pipeline takes about TODO days to run.

#### Recount tissue prediction
In the recount setting the multitissue classification analyses were trained on the 21 tissues (see supp. methods) that had at least 10 studies in the dataset.
Each model was trained to determine which of the 21 tissues a given expression sample corresponded to.
The models' performance was then measuerd based on the balanced accuracy across all classes.

The binary classification setting was similar.
The five tissues with the most studies (brain, blood, breast, stem cell, and cervix) were compared against each other pairwise.
The expression used in this setting was the set of samples labled as one of the two tissues being compared.

The data for both settings was split studywise.

#### GTEx classification
The multitissue classification analysis for GTEx used all 31 tissues.
Both the multiclass and binary settings were formulated and evaluated in the same way as in the recount data.
However, the data were split by tissue donor rather than by study.

#### Simulated data classification/sex prediction
The sex prediction and simulated data classification tasks were solely binary.
Both settings used balanced accuracy, as in the Recount3 and GTEx problems. 

#### Pretraining
In order to test the effects of pretraining on the different model types, we used an experimental setup that split the data into three sets.
Approximately forty percent of the data went into the pretraining set, forty percent went into the training set, and twenty percent went into the validation set.
The data was split such that each study's samples were in only one of the three sets, to simulate the real-world scenario where a model is trained on a publicly available data then fine-tuned on a dataset of interest.

To evaluate the models, we made two copies of each model with the same weight initialization.
The first copy was trained solely on the training data, while the second was trained on the pretraining data, then the training data.
Both models were then evaluated on the validation set.
This process was then repeated four more times with different studies assigned to the pretraining, training, and validation sets.

